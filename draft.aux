\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kaplan2020scalinglawsneurallanguage}
\citation{kaplan2020scalinglawsneurallanguage,hoffmann2022trainingcomputeoptimallargelanguage}
\citation{10.1371/journal.pone.0017514}
\citation{doi:10.1097/00004647-200110000-00001}
\@writefile{toc}{\contentsline {part}{Introduction}{3}{chapter*.2}\protected@file@percent }
\citation{Xu2024}
\citation{McCulloch1943}
\citation{Rosenblatt1958}
\@writefile{toc}{\contentsline {part}{Part I: Foundations of Biological Learning Models}{6}{part*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Neuron Models and Classical Learning Rules}{6}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Artificial Neuron Model}{6}{section.1.1}\protected@file@percent }
\citation{Baldi2021}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Classical Learning Rules}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:learning_rules}{{1.2}{7}{Classical Learning Rules}{section.1.2}{}}
\citation{Hopfield1982,Hopfield1984}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The Hopfield Network}{10}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Framework}{10}{section.2.1}\protected@file@percent }
\citation{Hertz2018}
\newlabel{eq:energy}{{2.3}{11}{Framework}{equation.2.3}{}}
\citation{Hertz2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Attractor states}{12}{section.2.2}\protected@file@percent }
\citation{PhysRevA.32.1007}
\citation{PhysRevLett.55.1530}
\citation{LITTLE1974101,Hinton1983,Peretto1984}
\citation{Hertz2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Capacity considerationss}{14}{section.2.3}\protected@file@percent }
\citation{PhysRevLett.55.1530,PhysRevA.35.2293,Amit1989}
\newlabel{eq:alpha_c}{{2.8}{15}{2.\ Extensive Recall: linear loading ratio}{equation.2.8}{}}
\citation{Sompolinsky1988,Crisanti1987,Derrida1987,Xu1996}
\citation{Parisi_1986,Hertz1986,Chengxiang2000}
\citation{Scardecchia2023,Student2024}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Random Recurrent Neural Networks}{16}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:random_rnn}{{3}{16}{Random Recurrent Neural Networks}{chapter.3}{}}
\citation{Student2024}
\citation{Scardecchia2023}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Stabilization through Self-couplings}{17}{section.3.1}\protected@file@percent }
\newlabel{sec: sc_stabilization}{{3.1}{17}{Stabilization through Self-couplings}{section.3.1}{}}
\newlabel{eq:Self_coupling_update}{{3.1}{17}{Stabilization through Self-couplings}{equation.3.1}{}}
\citation{Student2024}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Behavior of the critical self-coupling $J^*_D$ as a function of the network size $N$}}{19}{figure.caption.17}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:critical_JD_vs_Neurons}{{3.1}{19}{Behavior of the critical self-coupling $J^*_D$ as a function of the network size $N$}{figure.caption.17}{}}
\citation{Scholl2020,Forsythe2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Convergence times as a function of the self-coupling $J_D$}}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:average_convergence_time_vs_JD}{{3.2}{20}{Convergence times as a function of the self-coupling $J_D$}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Stabilization through Sparse Positive Reinforcement}{21}{section.3.2}\protected@file@percent }
\newlabel{eq:reinforcement_update}{{3.3}{21}{Stabilization through Sparse Positive Reinforcement}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Experimental Setup}{21}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Results}{22}{subsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparison of average iterations for networks of size 100, 1000, and 10000.  On the x-axis we have the reinforcement strength \(a\) and on the y-axis the exponential $d$ determing the order $O\left (N^d\right )$ of the sparsity}}{22}{figure.caption.20}\protected@file@percent }
\newlabel{fig:avg_iterations_comparison}{{3.3}{22}{Comparison of average iterations for networks of size 100, 1000, and 10000.\\ On the x-axis we have the reinforcement strength \(a\) and on the y-axis the exponential $d$ determing the order $O\left (N^d\right )$ of the sparsity}{figure.caption.20}{}}
\citation{TANAKA2019100}
\citation{Maass2002}
\citation{Jaeger2001}
\citation{Maass2002,Maass2004,Maass2011}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Reservoir Computing}{24}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Liquid State Machines}{24}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Reservoir Computing framework. The reservoir is a fixed recurrent neural network that transforms input data into a high-dimensional space. The readout layer is trained to perform the specific task.}}{25}{figure.caption.21}\protected@file@percent }
\newlabel{fig:rc_framework}{{4.1}{25}{Reservoir Computing framework. The reservoir is a fixed recurrent neural network that transforms input data into a high-dimensional space. The readout layer is trained to perform the specific task}{figure.caption.21}{}}
\citation{TANAKA2019100}
\citation{TANAKA2019100}
\citation{Melandri2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Echo State Networks}{26}{section.4.2}\protected@file@percent }
\citation{Cover1965}
\citation{Student2024}
\@writefile{toc}{\contentsline {part}{Part II: Multi-Layer Chain Models}{29}{part*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Introduction and Motivation}{29}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Background}{29}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces RNNs in parallel with spatial ferromagnetic connections}}{30}{figure.caption.26}\protected@file@percent }
\newlabel{fig:diagram_spatial_ferromagnetic}{{5.1}{30}{RNNs in parallel with spatial ferromagnetic connections}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Overview of Multi-Layer Chain Architecture}{31}{section.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Diagram of MLCM architecture. The input data is passed through the network and ends up at the output layer, which is connected to a classifier head}}{32}{figure.caption.27}\protected@file@percent }
\newlabel{fig: MLCM}{{5.2}{32}{Diagram of MLCM architecture. The input data is passed through the network and ends up at the output layer, which is connected to a classifier head}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Mathematical Formulation}{33}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Model Framework}{33}{section.6.1}\protected@file@percent }
\newlabel{eq:si_update}{{6.1}{34}{Model Framework}{equation.6.1}{}}
\newlabel{eq:yk_update}{{6.2}{34}{Model Framework}{equation.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Learning Dynamics}{34}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Convergence Step}{35}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Update Step}{36}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Empirical Evaluation}{37}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Baseline Implementation with Classical datasets}{38}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Experimental Setup}{38}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Results}{39}{subsection.7.1.2}\protected@file@percent }
\newlabel{fig:mnist_exp1}{{7.1a}{39}{\relax }{figure.caption.31}{}}
\newlabel{sub@fig:mnist_exp1}{{a}{39}{\relax }{figure.caption.31}{}}
\newlabel{fig:emnist_exp1}{{7.1b}{39}{\relax }{figure.caption.31}{}}
\newlabel{sub@fig:emnist_exp1}{{b}{39}{\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Comparison of testing and validation accuracy per epoch, reported with their $95\%$ confidence interval, for MLCM and the Perceptron (Single Layer Network - SLN) on MNIST (Fig. \ref {fig:mnist_exp1}) and Projected MNIST (Fig. \ref {fig:emnist_exp1}).}}{39}{figure.caption.31}\protected@file@percent }
\newlabel{fig:all_mnist_exp1}{{7.1}{39}{Comparison of testing and validation accuracy per epoch, reported with their $95\%$ confidence interval, for MLCM and the Perceptron (Single Layer Network - SLN) on MNIST (Fig. \ref {fig:mnist_exp1}) and Projected MNIST (Fig. \ref {fig:emnist_exp1})}{figure.caption.31}{}}
\newlabel{fig:fashion_mnist_exp1}{{7.2a}{40}{\relax }{figure.caption.32}{}}
\newlabel{sub@fig:fashion_mnist_exp1}{{a}{40}{\relax }{figure.caption.32}{}}
\newlabel{fig:efashion_mnist_exp1}{{7.2b}{40}{\relax }{figure.caption.32}{}}
\newlabel{sub@fig:efashion_mnist_exp1}{{b}{40}{\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Comparison of testing and validation accuracy per epoch, reported with their $95\%$ confidence interval, for MLCM and the Perceptron (Single Layer Network - SLN) on Fashion MNIST (Fig. \ref {fig:fashion_mnist_exp1}) and Projected Fashion MNIST (Fig. \ref {fig:efashion_mnist_exp1}).}}{40}{figure.caption.32}\protected@file@percent }
\newlabel{fig:all_fashion_mnist_exp1}{{7.2}{40}{Comparison of testing and validation accuracy per epoch, reported with their $95\%$ confidence interval, for MLCM and the Perceptron (Single Layer Network - SLN) on Fashion MNIST (Fig. \ref {fig:fashion_mnist_exp1}) and Projected Fashion MNIST (Fig. \ref {fig:efashion_mnist_exp1})}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Sparsity Enforcement in MLCMs}{40}{section.7.2}\protected@file@percent }
\citation{JAASKELAINEN2022119633}
\newlabel{eq:si_sparse_update}{{7.1}{41}{Sparsity Enforcement in MLCMs}{equation.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Fixed Threshold Sparsity}{42}{subsection.7.2.1}\protected@file@percent }
\newlabel{fig:mnist_exp2_1}{{7.3a}{43}{\relax }{figure.caption.35}{}}
\newlabel{sub@fig:mnist_exp2_1}{{a}{43}{\relax }{figure.caption.35}{}}
\newlabel{fig:mnist_accs_exp2_1}{{7.3b}{43}{\relax }{figure.caption.35}{}}
\newlabel{sub@fig:mnist_accs_exp2_1}{{b}{43}{\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Fig. \ref {fig:mnist_exp2_1} displays the sparsity of each layer in the final epoch of training and the final training and validation accuracy for each value of $\gamma $, reported with a $95\%$ confidence interval. Fig. \ref {fig:mnist_accs_exp2_1} shows the training and validation accuracy per epoch for each value of $\gamma $, reported with a $95\%$ confidence interval.}}{43}{figure.caption.35}\protected@file@percent }
\newlabel{fig:exp2_1}{{7.3}{43}{Fig. \ref {fig:mnist_exp2_1} displays the sparsity of each layer in the final epoch of training and the final training and validation accuracy for each value of $\gamma $, reported with a $95\%$ confidence interval. Fig. \ref {fig:mnist_accs_exp2_1} shows the training and validation accuracy per epoch for each value of $\gamma $, reported with a $95\%$ confidence interval}{figure.caption.35}{}}
\citation{Marx2015,Carandini2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Winner-Takes-All Sparsity}{44}{subsection.7.2.2}\protected@file@percent }
\bibstyle{plain}
\bibdata{reference}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusions and Future Directions}{46}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Summary of Contributions}{46}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Limitations and Open Questions}{46}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{References}{46}{section.8.2}\protected@file@percent }
\bibcite{Amit1989}{1}
\bibcite{PhysRevA.32.1007}{2}
\bibcite{PhysRevLett.55.1530}{3}
\bibcite{PhysRevA.35.2293}{4}
\bibcite{doi:10.1097/00004647-200110000-00001}{5}
\bibcite{Baldi2021}{6}
\bibcite{Carandini2011}{7}
\bibcite{Chengxiang2000}{8}
\bibcite{Cover1965}{9}
\bibcite{Crisanti1987}{10}
\bibcite{Student2024}{11}
\bibcite{Derrida1987}{12}
\bibcite{Forsythe2013}{13}
\bibcite{10.1371/journal.pone.0017514}{14}
\bibcite{Hertz1986}{15}
\bibcite{Hertz2018}{16}
\bibcite{Hinton1983}{17}
\bibcite{hoffmann2022trainingcomputeoptimallargelanguage}{18}
\bibcite{Hopfield1982}{19}
\bibcite{Hopfield1984}{20}
\bibcite{Jaeger2001}{21}
\bibcite{JAASKELAINEN2022119633}{22}
\bibcite{kaplan2020scalinglawsneurallanguage}{23}
\bibcite{LITTLE1974101}{24}
\bibcite{Maass2011}{25}
\bibcite{Maass2004}{26}
\bibcite{Maass2002}{27}
\bibcite{Marx2015}{28}
\bibcite{McCulloch1943}{29}
\bibcite{Melandri2014}{30}
\bibcite{Parisi_1986}{31}
\bibcite{Peretto1984}{32}
\bibcite{Rosenblatt1958}{33}
\bibcite{Scardecchia2023}{34}
\bibcite{Scholl2020}{35}
\bibcite{Sompolinsky1988}{36}
\bibcite{TANAKA2019100}{37}
\bibcite{Xu2024}{38}
\bibcite{Xu1996}{39}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}MLCM Training Algorithm}{51}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:mlcm_training_algorithm}{{A}{51}{MLCM Training Algorithm}{appendix.A}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces MLCM Training Algorithm}}{51}{algorithm.1}\protected@file@percent }
\newlabel{alg:forward_mlmc}{{1}{51}{MLCM Training Algorithm}{algorithm.1}{}}
\newlabel{line:call_conversion_dynamics}{{9}{51}{MLCM Training Algorithm}{algorithm.1}{}}
\newlabel{line:call_update_sweep}{{10}{51}{MLCM Training Algorithm}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces ConversionDynamics}}{52}{algorithm.2}\protected@file@percent }
\newlabel{alg:conversion_dynamics}{{2}{52}{ConversionDynamics}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces UpdateSweep}}{52}{algorithm.3}\protected@file@percent }
\newlabel{alg:training_pass}{{3}{52}{UpdateSweep}{algorithm.3}{}}
\gdef \@abspage@last{56}
