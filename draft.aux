\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{kaplan2020scalinglawsneurallanguage}
\citation{kaplan2020scalinglawsneurallanguage,hoffmann2022trainingcomputeoptimallargelanguage}
\citation{10.1371/journal.pone.0017514}
\citation{doi:10.1097/00004647-200110000-00001}
\@writefile{toc}{\contentsline {part}{Introduction}{3}{chapter*.2}\protected@file@percent }
\citation{Xu2024}
\citation{McCulloch1943}
\citation{Rosenblatt1958}
\@writefile{toc}{\contentsline {part}{Part I: Foundations of Biological Learning Models}{6}{part*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Neuron Models and Classical Learning Rules}{6}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Artificial Neuron Model}{6}{section.1.1}\protected@file@percent }
\citation{Baldi2021}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Classical Learning Rules}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:learning_rules}{{1.2}{7}{Classical Learning Rules}{section.1.2}{}}
\citation{Hopfield1982,Hopfield1984}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The Hopfield Network}{10}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Framework}{10}{section.2.1}\protected@file@percent }
\citation{Hertz2018}
\newlabel{eq:energy}{{2.3}{11}{Framework}{equation.2.3}{}}
\citation{Hertz2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Attractor states}{12}{section.2.2}\protected@file@percent }
\citation{PhysRevA.32.1007}
\citation{PhysRevLett.55.1530}
\citation{LITTLE1974101,Hinton1983,Peretto1984}
\citation{Hertz2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Capacity considerationss}{14}{section.2.3}\protected@file@percent }
\citation{PhysRevLett.55.1530,PhysRevA.35.2293,Amit1989}
\newlabel{eq:alpha_c}{{2.8}{15}{2.\ Extensive Recall: linear loading ratio}{equation.2.8}{}}
\citation{Sompolinsky1988,Crisanti1987,Derrida1987,Xu1996}
\citation{Parisi_1986,Hertz1986,Chengxiang2000}
\citation{Scardecchia2023,Student2024}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Random Recurrent Neural Networks}{16}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:random_rnn}{{3}{16}{Random Recurrent Neural Networks}{chapter.3}{}}
\citation{Student2024}
\citation{Scardecchia2023}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Stabilization through Self-couplings}{17}{section.3.1}\protected@file@percent }
\newlabel{sec: sc_stabilization}{{3.1}{17}{Stabilization through Self-couplings}{section.3.1}{}}
\newlabel{eq:Self_coupling_update}{{3.1}{17}{Stabilization through Self-couplings}{equation.3.1}{}}
\citation{Student2024}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Behavior of the critical self-coupling $J^*_D$ as a function of the network size $N$}}{19}{figure.caption.17}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:critical_JD_vs_Neurons}{{3.1}{19}{Behavior of the critical self-coupling $J^*_D$ as a function of the network size $N$}{figure.caption.17}{}}
\citation{Scholl2020,Forsythe2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Convergence times as a function of the self-coupling $J_D$}}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:average_convergence_time_vs_JD}{{3.2}{20}{Convergence times as a function of the self-coupling $J_D$}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Stabilization through Sparse Positive Reinforcement}{21}{section.3.2}\protected@file@percent }
\newlabel{eq:reinforcement_update}{{3.3}{21}{Stabilization through Sparse Positive Reinforcement}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Experimental Setup}{21}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Results}{22}{subsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Comparison of average iterations for networks of size 100, 1000, and 10000.  On the x-axis we have the reinforcement strength \(a\) and on the y-axis the exponential $d$ determing the order $O\left (N^d\right )$ of the sparsity}}{22}{figure.caption.20}\protected@file@percent }
\newlabel{fig:avg_iterations_comparison}{{3.3}{22}{Comparison of average iterations for networks of size 100, 1000, and 10000.\\ On the x-axis we have the reinforcement strength \(a\) and on the y-axis the exponential $d$ determing the order $O\left (N^d\right )$ of the sparsity}{figure.caption.20}{}}
\citation{TANAKA2019100}
\citation{Maass2002}
\citation{Jaeger2001}
\citation{Maass2002,Maass2004,Maass2011}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Reservoir Computing}{24}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Liquid State Machines}{24}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Reservoir Computing framework. The reservoir is a fixed recurrent neural network that transforms input data into a high-dimensional space. The readout layer is trained to perform the specific task.}}{25}{figure.caption.21}\protected@file@percent }
\newlabel{fig:rc_framework}{{4.1}{25}{Reservoir Computing framework. The reservoir is a fixed recurrent neural network that transforms input data into a high-dimensional space. The readout layer is trained to perform the specific task}{figure.caption.21}{}}
\citation{TANAKA2019100}
\citation{TANAKA2019100}
\citation{Melandri2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Echo State Networks}{26}{section.4.2}\protected@file@percent }
\citation{Cover1965}
\citation{Student2024}
\@writefile{toc}{\contentsline {part}{Part II: Multi-Layer Chain Models}{29}{part*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Introduction and Motivation}{29}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Background}{29}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces RNNs in parallel with spatial ferromagnetic connections}}{30}{figure.caption.26}\protected@file@percent }
\newlabel{fig:diagram_spatial_ferromagnetic}{{5.1}{30}{RNNs in parallel with spatial ferromagnetic connections}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Overview of Multi-Layer Chain Architecture}{31}{section.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Diagram of MLCM architecture. The input data is passed through the network and ends up at the output layer, which is connected to a classifier head}}{31}{figure.caption.27}\protected@file@percent }
\newlabel{fig: MLCM}{{5.2}{31}{Diagram of MLCM architecture. The input data is passed through the network and ends up at the output layer, which is connected to a classifier head}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Mathematical Formulation}{32}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Model Framework}{32}{section.6.1}\protected@file@percent }
\newlabel{eq:si_update}{{6.1}{33}{Model Framework}{equation.6.1}{}}
\newlabel{eq:yk_update}{{6.2}{33}{Model Framework}{equation.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Learning Dynamics}{33}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Convergence Step}{34}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Update Step}{35}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Empirical Evaluation}{36}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Baseline Implementation with MNIST dataset}{36}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Experimental Setup}{36}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Results}{36}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Going Beyond the Perceptron}{36}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Experimental Setup}{36}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Results}{36}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Enforcing Sparsity in MLCMs}{36}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Experimental Setup}{36}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Results}{36}{subsection.7.3.2}\protected@file@percent }
\bibstyle{plain}
\bibdata{reference}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusions and Future Directions}{37}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Summary of Contributions}{37}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Limitations and Open Questions}{37}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{References}{37}{section.8.2}\protected@file@percent }
\bibcite{Amit1989}{1}
\bibcite{PhysRevA.32.1007}{2}
\bibcite{PhysRevLett.55.1530}{3}
\bibcite{PhysRevA.35.2293}{4}
\bibcite{doi:10.1097/00004647-200110000-00001}{5}
\bibcite{Baldi2021}{6}
\bibcite{Chengxiang2000}{7}
\bibcite{Cover1965}{8}
\bibcite{Crisanti1987}{9}
\bibcite{Derrida1987}{10}
\bibcite{Forsythe2013}{11}
\bibcite{10.1371/journal.pone.0017514}{12}
\bibcite{Hertz1986}{13}
\bibcite{Hertz2018}{14}
\bibcite{Hinton1983}{15}
\bibcite{hoffmann2022trainingcomputeoptimallargelanguage}{16}
\bibcite{Hopfield1982}{17}
\bibcite{Hopfield1984}{18}
\bibcite{Jaeger2001}{19}
\bibcite{kaplan2020scalinglawsneurallanguage}{20}
\bibcite{LITTLE1974101}{21}
\bibcite{Maass2011}{22}
\bibcite{Maass2004}{23}
\bibcite{Maass2002}{24}
\bibcite{McCulloch1943}{25}
\bibcite{Melandri2014}{26}
\bibcite{Parisi_1986}{27}
\bibcite{Peretto1984}{28}
\bibcite{Rosenblatt1958}{29}
\bibcite{Saglietti2018}{30}
\bibcite{Scardecchia2023}{31}
\bibcite{Scholl2020}{32}
\bibcite{Sompolinsky1988}{33}
\bibcite{TANAKA2019100}{34}
\bibcite{Student2024}{35}
\bibcite{Xu2024}{36}
\bibcite{Xu1996}{37}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}MLCM Training Algorithm}{42}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:mlcm_training_algorithm}{{A}{42}{MLCM Training Algorithm}{appendix.A}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces MLCM Training Algorithm}}{42}{algorithm.1}\protected@file@percent }
\newlabel{alg:forward_mlmc}{{1}{42}{MLCM Training Algorithm}{algorithm.1}{}}
\newlabel{line:call_conversion_dynamics}{{9}{42}{MLCM Training Algorithm}{algorithm.1}{}}
\newlabel{line:call_update_sweep}{{10}{42}{MLCM Training Algorithm}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces ConversionDynamics}}{43}{algorithm.2}\protected@file@percent }
\newlabel{alg:conversion_dynamics}{{2}{43}{ConversionDynamics}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces UpdateSweep}}{43}{algorithm.3}\protected@file@percent }
\newlabel{alg:training_pass}{{3}{43}{UpdateSweep}{algorithm.3}{}}
\gdef \@abspage@last{47}
