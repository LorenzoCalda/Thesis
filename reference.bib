@inproceedings{strubell-etal-2019-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355/",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."
}

@article{doi:10.1097/00004647-200110000-00001,
    author = {David Attwell and Simon B. Laughlin},
    title ={An Energy Budget for Signaling in the Grey Matter of the Brain},
    journal = {Journal of Cerebral Blood Flow \& Metabolism},
    volume = {21},
    number = {10},
    pages = {1133-1145},
    year = {2001},
    doi = {10.1097/00004647-200110000-00001},
        note ={PMID: 11598490},
    URL = { 
            https://doi.org/10.1097/00004647-200110000-00001
    },
    eprint = { 
            https://doi.org/10.1097/00004647-200110000-00001
    },
    abstract = { Anatomic and physiologic data are used to analyze the energy expenditure on different components of excitatory signaling in the grey matter of rodent brain. Action potentials and postsynaptic effects of glutamate are predicted to consume much of the energy (47\% and 34\%, respectively), with the resting potential consuming a smaller amount (13\%), and glutamate recycling using only 3\%. Energy usage depends strongly on action potential rate—an increase in activity of 1 action potential/cortical neuron/s will raise oxygen consumption by 145 mL/100 g grey matter/h. The energy expended on signaling is a large fraction of the total energy used by the brain; this favors the use of energy efficient neural codes and wiring patterns. Our estimates of energy usage predict the use of distributed codes, with ≤15\% of neurons simultaneously active, to reduce energy consumption and allow greater computing power from a fixed number of neurons. Functional magnetic resonance imaging signals are likely to be dominated by changes in energy usage associated with synaptic currents and action potential propagation. }
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@article{10.1371/journal.pone.0017514,
    doi = {10.1371/journal.pone.0017514},
    author = {Herculano-Houzel, Suzana},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Scaling of Brain Metabolism with a Fixed Energy Budget per Neuron: Implications for Neuronal Activity, Plasticity and Evolution},
    year = {2011},
    month = {03},
    volume = {6},
    url = {https://doi.org/10.1371/journal.pone.0017514},
    pages = {1-9},
    abstract = {It is usually considered that larger brains have larger neurons, which consume more energy individually, and are therefore accompanied by a larger number of glial cells per neuron. These notions, however, have never been tested. Based on glucose and oxygen metabolic rates in awake animals and their recently determined numbers of neurons, here I show that, contrary to the expected, the estimated glucose use per neuron is remarkably constant, varying only by 40% across the six species of rodents and primates (including humans). The estimated average glucose use per neuron does not correlate with neuronal density in any structure. This suggests that the energy budget of the whole brain per neuron is fixed across species and brain sizes, such that total glucose use by the brain as a whole, by the cerebral cortex and also by the cerebellum alone are linear functions of the number of neurons in the structures across the species (although the average glucose consumption per neuron is at least 10× higher in the cerebral cortex than in the cerebellum). These results indicate that the apparently remarkable use in humans of 20% of the whole body energy budget by a brain that represents only 2% of body mass is explained simply by its large number of neurons. Because synaptic activity is considered the major determinant of metabolic cost, a conserved energy budget per neuron has several profound implications for synaptic homeostasis and the regulation of firing rates, synaptic plasticity, brain imaging, pathologies, and for brain scaling in evolution.},
    number = {3},

}

@article{Xu2024,
  title = {Inspires effective alternatives to backpropagation: predictive coding helps understand and build learning},
  volume = {20},
  ISSN = {1876-7958},
  url = {http://dx.doi.org/10.4103/NRR.NRR-D-24-00629},
  DOI = {10.4103/nrr.nrr-d-24-00629},
  number = {11},
  journal = {Neural Regeneration Research},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
  author = {Xu,  Zhenghua and Yu,  Miao and Song,  Yuhang},
  year = {2024},
  month = oct,
  pages = {3215-3216}
}

@article{McCulloch1943,
  title = {A logical calculus of the ideas immanent in nervous activity},
  volume = {5},
  ISSN = {1522-9602},
  url = {http://dx.doi.org/10.1007/BF02478259},
  DOI = {10.1007/bf02478259},
  number = {4},
  journal = {The Bulletin of Mathematical Biophysics},
  publisher = {Springer Science and Business Media LLC},
  author = {McCulloch,  Warren S. and Pitts,  Walter},
  year = {1943},
  month = dec,
  pages = {115-133}
}

@article{Rosenblatt1958,
  title = {The perceptron: A probabilistic model for information storage and organization in the brain.},
  volume = {65},
  ISSN = {0033-295X},
  url = {http://dx.doi.org/10.1037/h0042519},
  DOI = {10.1037/h0042519},
  number = {6},
  journal = {Psychological Review},
  publisher = {American Psychological Association (APA)},
  author = {Rosenblatt,  F.},
  year = {1958},
  pages = {386-408}
}

@book{Baldi2021,
  title = {Deep Learning in Science},
  ISBN = {9781108845359},
  url = {http://dx.doi.org/10.1017/9781108955652},
  DOI = {10.1017/9781108955652},
  publisher = {Cambridge University Press},
  author = {Baldi,  Pierre},
  year = {2021},
  month = apr 
}

@article{Hopfield1982,
  title = {Neural networks and physical systems with emergent collective computational abilities.},
  volume = {79},
  ISSN = {1091-6490},
  url = {http://dx.doi.org/10.1073/pnas.79.8.2554},
  DOI = {10.1073/pnas.79.8.2554},
  number = {8},
  journal = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author = {Hopfield,  J.J.},
  year = {1982},
  month = apr,
  pages = {2554-2558}
}

@article{Hopfield1984,
 ISSN = {00278424},
 URL = {http://www.jstor.org/stable/23632},
 abstract = {A model for a large network of ``neurons'' with a graded response (or sigmoid input--output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch--Pitts neurons. The content-addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological ``neurons.'' Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
 author = {Hopfield,  J.J.},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {10},
 pages = {3088--3092},
 publisher = {National Academy of Sciences},
 title = {Neurons with Graded Response Have Collective Computational Properties like Those of Two-State Neurons},
 urldate = {2025-04-24},
 volume = {81},
 year = {1984}
}

@book{Hertz2018,
  title = {Introduction to the Theory of Neural Computation},
  ISBN = {9780429499661},
  url = {http://dx.doi.org/10.1201/9780429499661},
  DOI = {10.1201/9780429499661},
  publisher = {CRC Press},
  author = {Hertz,  John and Krogh,  Anders and Palmer,  Richard G.},
  year = {2018},
  month = mar 
}

@article{PhysRevA.32.1007,
  title = {Spin-glass models of neural networks},
  author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, H.},
  journal = {Phys. Rev. A},
  volume = {32},
  issue = {2},
  pages = {1007--1018},
  numpages = {0},
  year = {1985},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.32.1007},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.32.1007}
}

@article{PhysRevLett.55.1530,
  title = {Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks},
  author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, H.},
  journal = {Phys. Rev. Lett.},
  volume = {55},
  issue = {14},
  pages = {1530--1533},
  numpages = {0},
  year = {1985},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.55.1530},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.55.1530}
}

@article{LITTLE1974101,
title = {The existence of persistent states in the brain},
journal = {Mathematical Biosciences},
volume = {19},
number = {1},
pages = {101-120},
year = {1974},
issn = {0025-5564},
doi = {https://doi.org/10.1016/0025-5564(74)90031-5},
url = {https://www.sciencedirect.com/science/article/pii/0025556474900315},
author = {W.A. Little},
abstract = {We show that given certain plausible assumptions the existence of persistent states in a neural network can occur only if a certain transfer matrix has degenerate maximum eigenvalues. The existence of such states of persistent order is directly analogous to the existence of long range order in an Ising spin system; while the transition to the state of persistent order is analogous to the transition to the ordered phase of the spin system. It is shown that the persistent state is also characterized by correlations between neurons throughout the brain. It is suggested that these persistent states are associated with short term memory while the eigenvectors of the transfer matrix are a representation of long term memory. A numerical example is given that illustrates certain of these features.}
}

@InProceedings{Hinton1983,
  author     = {Hinton, G. E. and Sejnowski, T.},
  title      = {Optimal perceptual inference},
  booktitle  = cvpr83,
  year       = {1983},
  month      = jun,
  address   = {Washington, DC},
  pages     = {448-453},
  publisher = {ieeecs},
}

@article{Peretto1984,
  title = {Collective properties of neural networks: A statistical physics approach},
  volume = {50},
  ISSN = {1432-0770},
  url = {http://dx.doi.org/10.1007/BF00317939},
  DOI = {10.1007/bf00317939},
  number = {1},
  journal = {Biological Cybernetics},
  publisher = {Springer Science and Business Media LLC},
  author = {Peretto,  P.},
  year = {1984},
  month = feb,
  pages = {51-62}
}

@article{PhysRevA.35.2293,
  title = {Information storage in neural networks with low levels of activity},
  author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, H.},
  journal = {Phys. Rev. A},
  volume = {35},
  issue = {5},
  pages = {2293--2303},
  numpages = {0},
  year = {1987},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.35.2293},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.35.2293}
}

@book{Amit1989,
  title = {Modeling Brain Function: The World of Attractor Neural Networks},
  ISBN = {9780511623257},
  url = {http://dx.doi.org/10.1017/CBO9780511623257},
  DOI = {10.1017/cbo9780511623257},
  publisher = {Cambridge University Press},
  author = {Amit,  Daniel J.},
  year = {1989},
  month = sep 
}

@article{Maass2002,
  title = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
  volume = {14},
  ISSN = {1530-888X},
  url = {http://dx.doi.org/10.1162/089976602760407955},
  DOI = {10.1162/089976602760407955},
  number = {11},
  journal = {Neural Computation},
  publisher = {MIT Press - Journals},
  author = {Maass,  Wolfgang and Natschl\"{a}ger,  Thomas and Markram,  Henry},
  year = {2002},
  month = nov,
  pages = {25312560}
}

@inbook{Maass2011,
author = {Wolfgang Maass},
title = {Liquid State Machines: Motivation, Theory, and Applications},
booktitle = {Computability in Context},
year = {2011},
pages = {275-296},
doi = {10.1142/9781848162778_0008},
URL = {https://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008},
eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9781848162778_0008},
    abstract = { Abstract The Liquid State Machine (LSM) has emerged as a computational model that is more adequate than the Turing machine for describing computations in biological networks of neurons. Characteristic features of this new model are (i) that it is a model for adaptive computational systems, (ii) that it provides a method for employing randomly connected circuits, or even “found” physical objects for meaningful computations, (iii) that it provides a theoretical context where heterogeneous, rather than stereo typical, local gates, or processors increase the computational power of a circuit, (iv) that it provides a method for multiplexing different computations (on a common input) within the same circuit. This chapter reviews the motivation for this model, its theoretical background, and current work on implementations of this model in innovative artificial computing devices. }
}

@article{Maass2004,
title = {On the computational power of circuits of spiking neurons},
journal = {Journal of Computer and System Sciences},
volume = {69},
number = {4},
pages = {593-616},
year = {2004},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2004.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022000004000406},
author = {Wolfgang Maass and Henry Markram},
abstract = {Complex real-time computations on multi-modal time-varying input streams are carried out by generic cortical microcircuits. Obstacles for the development of adequate theoretical models that could explain the seemingly universal power of cortical microcircuits for real-time computing are the complexity and diversity of their computational units (neurons and synapses), as well as the traditional emphasis on offline computing in almost all theoretical approaches towards neural computation. In this article, we initiate a rigorous mathematical analysis of the real-time computing capabilities of a new generation of models for neural computation, liquid state machines, that can be implemented with—in fact benefit from—diverse computational units. Hence, realistic models for cortical microcircuits represent special instances of such liquid state machines, without any need to simplify or homogenize their diverse computational units. We present proofs of two theorems about the potential computational power of such models for real-time computing, both on analog input streams and for spike trains as inputs.}
}

@article{Jaeger2001,
author = {Jaeger, Herbert},
year = {2001},
month = {01},
pages = {},
title = {The" echo state" approach to analysing and training recurrent neural networks-with an erratum note'},
volume = {148},
journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

@article{TANAKA2019100,
title = {Recent advances in physical reservoir computing: A review},
journal = {Neural Networks},
volume = {115},
pages = {100-123},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300784},
author = {Gouhei Tanaka and Toshiyuki Yamane and Jean Benoit Héroux and Ryosho Nakane and Naoki Kanazawa and Seiji Takeda and Hidetoshi Numata and Daiju Nakano and Akira Hirose},
keywords = {Neural networks, Machine learning, Reservoir computing, Nonlinear dynamical systems, Neuromorphic device},
abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.}
}

@mastersthesis{Melandri2014,
  author = {Luca Melandri},
  school = {Alma Mater Studiorum - Università di Bologna},
  title = {Introduction to Reservoir Computing Methods},
  year = {2014}
}

@article{Parisi_1986,
doi = {10.1088/0305-4470/19/11/005},
url = {https://dx.doi.org/10.1088/0305-4470/19/11/005},
year = {1986},
month = {aug},
publisher = {},
volume = {19},
number = {11},
pages = {L675},
author = {G Parisi},
title = {Asymmetric neural networks and the process of learning},
journal = {Journal of Physics A: Mathematical and General},
abstract = {Studies the influence of a strong asymmetry of the synaptic strengths on the behavior of a neural network which works as an associative memory. The author finds that the asymmetry in the synaptic strengths may be crucial for the process of learning.}
}

@inproceedings{Hertz1986,
  title = {Memory networks with asymmetric bonds},
  volume = {151},
  ISSN = {0094-243X},
  url = {http://dx.doi.org/10.1063/1.36259},
  DOI = {10.1063/1.36259},
  booktitle = {AIP Conference Proceedings},
  publisher = {AIP},
  author = {Hertz,  J. A. and Grinstein,  G. and Solla,  S. A.},
  year = {1986},
  pages = {212-218}
}

@article{Chengxiang2000,
  title = {Retrieval Properties of a Hopfield Model with Random Asymmetric Interactions},
  volume = {12},
  ISSN = {1530-888X},
  url = {http://dx.doi.org/10.1162/089976600300015628},
  DOI = {10.1162/089976600300015628},
  number = {4},
  journal = {Neural Computation},
  publisher = {MIT Press - Journals},
  author = {Chengxiang,  Zhang and Dasgupta,  Chandan and Singh,  Manoranjan P.},
  year = {2000},
  month = apr,
  pages = {865-880}
}

@article{Sompolinsky1988,
  title = {Chaos in Random Neural Networks},
  volume = {61},
  ISSN = {0031-9007},
  url = {http://dx.doi.org/10.1103/PhysRevLett.61.259},
  DOI = {10.1103/physrevlett.61.259},
  number = {3},
  journal = {Physical Review Letters},
  publisher = {American Physical Society (APS)},
  author = {Sompolinsky,  H. and Crisanti,  A. and Sommers,  H. J.},
  year = {1988},
  month = jul,
  pages = {259-262}
}

@article{Crisanti1987,
  title = {Dynamics of spin systems with randomly asymmetric bonds: Langevin dynamics and a spherical model},
  volume = {36},
  ISSN = {0556-2791},
  url = {http://dx.doi.org/10.1103/PhysRevA.36.4922},
  DOI = {10.1103/physreva.36.4922},
  number = {10},
  journal = {Physical Review A},
  publisher = {American Physical Society (APS)},
  author = {Crisanti,  A. and Sompolinsky,  H.},
  year = {1987},
  month = nov,
  pages = {4922-4939}
}

@article{Derrida1987,
  title = {An Exactly Solvable Asymmetric Neural Network Model},
  volume = {4},
  ISSN = {1286-4854},
  url = {http://dx.doi.org/10.1209/0295-5075/4/2/007},
  DOI = {10.1209/0295-5075/4/2/007},
  number = {2},
  journal = {Europhysics Letters (EPL)},
  publisher = {IOP Publishing},
  author = {Derrida,  B and Gardner,  E and Zippelius,  A},
  year = {1987},
  month = jul,
  pages = {167-173}
}

@article{Xu1996,
  title = {Asymmetric Hopfield-type networks: Theory and applications},
  volume = {9},
  ISSN = {0893-6080},
  url = {http://dx.doi.org/10.1016/0893-6080(95)00114-X},
  DOI = {10.1016/0893-6080(95)00114-x},
  number = {3},
  journal = {Neural Networks},
  publisher = {Elsevier BV},
  author = {Xu,  Zong-Ben and Hu,  Guo-Qing and Kwong,  Chung-Ping},
  year = {1996},
  month = apr,
  pages = {483-501}
}

@mastersthesis{Scardecchia2023,
  author = {Scardecchia, Mattia},
  school = {Bocconi University},
  title = {Statistical Physics of Learning},
  type = {Bachelor's thesis based on unpublished work of {R}. {Zecchina}, {M}. {Mezard}, {C}. {Baldassi}},
  year = {2023},
}

@mastersthesis{Student2024,
  author = {Del Gaudio, Martina},
  school = {Bocconi University},
  title = {Unknown},
  type = {Bachelor's thesis based on unpublished work of {R}. {Zecchina}, {M}. {Mezard}, {C}. {Baldassi}},
  year = {2024}
}

@article{Scholl2020,
  title = {Cortical response selectivity derives from strength in numbers of synapses},
  volume = {590},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/s41586-020-03044-3},
  DOI = {10.1038/s41586-020-03044-3},
  number = {7844},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Scholl,  Benjamin and Thomas,  Connon I. and Ryan,  Melissa A. and Kamasawa,  Naomi and Fitzpatrick,  David},
  year = {2020},
  month = dec,
  pages = {111-114}
}

@article{Forsythe2013,
  title = {Size matters: formation and function of giant synapses},
  volume = {591},
  ISSN = {1469-7793},
  url = {http://dx.doi.org/10.1113/jphysiol.2013.258954},
  DOI = {10.1113/jphysiol.2013.258954},
  number = {13},
  journal = {The Journal of Physiology},
  publisher = {Wiley},
  author = {Forsythe,  Ian D. and Wu,  Chunlai and Borst,  J. Gerard G.},
  year = {2013},
  month = jul,
  pages = {3123–3123}
}

@ARTICLE{Cover1965,
  author={Cover, Thomas M.},
  journal={IEEE Transactions on Electronic Computers}, 
  title={Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition}, 
  year={1965},
  volume={EC-14},
  number={3},
  pages={326-334},
  keywords={Pattern recognition;Vectors;Application software;Boolean functions;Geometry;History},
  doi={10.1109/PGEC.1965.264137}
}

@article{Saglietti2018,
  title = {From statistical inference to a differential learning rule for stochastic neural networks},
  volume = {8},
  ISSN = {2042-8901},
  url = {http://dx.doi.org/10.1098/rsfs.2018.0033},
  DOI = {10.1098/rsfs.2018.0033},
  number = {6},
  journal = {Interface Focus},
  publisher = {The Royal Society},
  author = {Saglietti,  Luca and Gerace,  Federica and Ingrosso,  Alessandro and Baldassi,  Carlo and Zecchina,  Riccardo},
  year = {2018},
  month = oct,
  pages = {20180033}
}

@article{JAASKELAINEN2022119633,
title = {Do sparse brain activity patterns underlie human cognition?},
journal = {NeuroImage},
volume = {263},
pages = {119633},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119633},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922007480},
author = {Iiro P. Jääskeläinen and Enrico Glerean and Vasily Klucharev and Anna Shestakova and Jyrki Ahveninen},
keywords = {fMRI, MVPA, Sparse distributed representations, Perception, Emotions, Cognition},
abstract = {Accumulating multivariate pattern analysis (MVPA) results from fMRI studies suggest that information is represented in fingerprint patterns of activations and deactivations during perception, emotions, and cognition. We postulate that these fingerprint patterns might reflect neuronal-population level sparse code documented in two-photon calcium imaging studies in animal models, i.e., information represented in specific and reproducible ensembles of a few percent of active neurons amidst widespread inhibition in neural populations. We suggest that such representations constitute a fundamental organizational principle via interacting across multiple levels of brain hierarchy, thus giving rise to perception, emotions, and cognition.}
}

@article{Marx2015,
  title = {Competition with and without priority control: linking rivalry to attention through winner-take-all networks with memory},
  volume = {1339},
  ISSN = {1749-6632},
  url = {http://dx.doi.org/10.1111/nyas.12575},
  DOI = {10.1111/nyas.12575},
  number = {1},
  journal = {Annals of the New York Academy of Sciences},
  publisher = {Wiley},
  author = {Marx,  Svenja and Gruenhage,  Gina and Walper,  Daniel and Rutishauser,  Ueli and Einh\"{a}user,  Wolfgang},
  year = {2015},
  month = jan,
  pages = {138-153}
}

@article{Carandini2011,
  title = {Normalization as a canonical neural computation},
  volume = {13},
  ISSN = {1471-0048},
  url = {http://dx.doi.org/10.1038/nrn3136},
  DOI = {10.1038/nrn3136},
  number = {1},
  journal = {Nature Reviews Neuroscience},
  publisher = {Springer Science and Business Media LLC},
  author = {Carandini,  Matteo and Heeger,  David J.},
  year = {2011},
  month = nov,
  pages = {51-62}
}